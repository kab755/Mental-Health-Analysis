# -*- coding: utf-8 -*-
"""mentalH.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LR4NCCogXLZkRJ4Dwx6cYlzmwyAGpBCP
"""

#imports necessary libraries to do basic things on the dataset
import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt


print('Successfully imported')

#Reading data
data = pd.read_csv('/content/drive/MyDrive/survey.csv')
data.head()

#Check the dataset for missing data
if data.isnull().sum().sum() == 0 :
    print ('There is no missing data in our dataset')
else:
    print('There is {} missing data in our dataset '.format(data.isnull().sum().sum()))

#Check our missing data from which columns and how many unique features they have.
frame = pd.concat([data.isnull().sum(), data.nunique(), data.dtypes], axis = 1, sort= False)
frame

from sklearn.impute import SimpleImputer
import numpy as np
columns_to_drop = ['state', 'comments', 'Timestamp']
for column in columns_to_drop:
    if column in data.columns:
        data = data.drop(columns=[column])


# Fill in missing values in work_interfere column
data['work_interfere'] = np.ravel(SimpleImputer(strategy = 'most_frequent').fit_transform(data['work_interfere'].values.reshape(-1,1)))
data['self_employed'] = np.ravel(SimpleImputer(strategy = 'most_frequent').fit_transform(data['self_employed'].values.reshape(-1,1)))

data.head()

ax = sns.countplot(data=data, x='work_interfere');
ax.bar_label(ax.containers[0]);

#Check unique data in gender columns
print(data['Gender'].unique())
print('')
print('-'*75)
print('')
#Check number of unique data too.
print('number of unique Gender in our dataset is :', data['Gender'].nunique())

#Since we know there are only two genders and others lets categorise on basis of it

data['Gender'].replace(['Male ', 'male', 'M', 'm', 'Male', 'Cis Male',
                     'Man', 'cis male', 'Mail', 'Male-ish', 'Male (CIS)',
                      'Cis Man', 'msle', 'Malr', 'Mal', 'maile', 'Make',], 'Male', inplace = True)

data['Gender'].replace(['Female ', 'female', 'F', 'f', 'Woman', 'Female',
                     'femail', 'Cis Female', 'cis-female/femme', 'Femake', 'Female (cis)',
                     'woman',], 'Female', inplace = True)

data["Gender"].replace(['Female (trans)', 'queer/she/they', 'non-binary',
                     'fluid', 'queer', 'Androgyne', 'Trans-female', 'male leaning androgynous',
                      'Agender', 'A little about you', 'Nah', 'All',
                      'ostensibly male, unsure what that really means',
                      'Genderqueer', 'Enby', 'p', 'Neuter', 'something kinda male?',
                      'Guy (-ish) ^_^', 'Trans woman',], 'Other', inplace = True)

print(data['Gender'].unique())

ax = sns.countplot(data=data, x='Gender');
ax.bar_label(ax.containers[0]);

#Checking whether data is missing or not
if data.isnull().sum().sum() == 0:
    print('There is no missing data')
else:
    print('There is {} missing data'.format(data.isnull().sum().sum()))

#Let's check duplicated data.
if data.duplicated().sum() == 0:
    print('There is no duplicated data:')
else:
    print('There is {} duplicated data:'.format(data.duplicated().sum()))
    #If there is duplicated data drop it.
    data.drop_duplicates(inplace=True)

print('-'*50)
print(data.duplicated().sum())

data['Age'].unique()

#lets take data of age till 100 years and will remove negative values
data.drop(data[data['Age']<0].index, inplace = True)
data.drop(data[data['Age']>99].index, inplace = True)

print(data['Age'].unique())

#Let's see the Age distribution in this dataset.
plt.figure(figsize = (10,6))
age_range_plot = sns.countplot(data = data, x = 'Age');
age_range_plot.bar_label(age_range_plot.containers[0]);
plt.xticks(rotation=90);

"""DATA **ENCODING**"""

#In this plot We can see Total number of individuals who received treatment or not.
plt.figure(figsize = (10,6));
treat = sns.countplot(data = data,  x = 'treatment');
treat.bar_label(treat.containers[0]);
plt.title('Total number of individuals who received treatment or not');

data.info()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
#Make the dataset include all the columns we need to change their dtypes
columns_to_encode = ['Gender', 'Country', 'self_employed','family_history', 'treatment', 'work_interfere','no_employees',
                             'remote_work', 'tech_company','benefits','care_options', 'wellness_program',
                             'seek_help', 'anonymity', 'leave', 'mental_health_consequence', 'phys_health_consequence',
                             'coworkers', 'supervisor', 'mental_health_interview','phys_health_interview',
                             'mental_vs_physical', 'obs_consequence']
#Write a Loop for fitting LabelEncoder on columns_to_encode
for columns in columns_to_encode:
    data[columns] = le.fit_transform(data[columns])

data.info()

#sd
data.describe()

from sklearn.preprocessing import MaxAbsScaler, StandardScaler

data['Age'] = MaxAbsScaler().fit_transform(data[['Age']])
data['Country'] = StandardScaler().fit_transform(data[['Country']])
data['work_interfere'] = StandardScaler().fit_transform(data[['work_interfere']])
data['no_employees'] = StandardScaler().fit_transform(data[['no_employees']])
data['leave'] = StandardScaler().fit_transform(data[['leave']])

data.describe()

"""TRAINING ANG TESTING SPLIT

"""

from sklearn.model_selection import train_test_split

#I wanna work on 'treatment' column.
X = data.drop(columns = ['treatment'])
y = data['treatment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

print(X_train.shape, y_train.shape)
print('-'*30)
print(X_test.shape, y_test.shape)
print('_'*30)

from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.tree import DecisionTreeClassifier as DT
from sklearn.naive_bayes import GaussianNB

# Define the steps for the pipeline
steps_nb = [('Scaler', StandardScaler()),
            ('clf', GaussianNB())]

# Create the pipeline
clf_nb = Pipeline(steps=steps_nb)

# Fit the model
clf_nb.fit(X_train, y_train)

# Predict on the test set
y_pred_nb = clf_nb.predict(X_test)

# Calculate accuracy
accuracy_nb = accuracy_score(y_true=y_test, y_pred=y_pred_nb) * 100
print('Naive Bayes accuracy:', accuracy_nb)

!pip install shap

import shap

from sklearn.metrics import confusion_matrix

# ... (your existing code)

# After you have predicted the target variable 'y_pred_nb'
conf_matrix = confusion_matrix(y_test, y_pred_nb)

# Display the confusion matrix
print("Confusion Matrix:")
print(conf_matrix)

from sklearn.metrics import classification_report

report = classification_report(y_test, y_pred_nb)
print(report)

from sklearn.metrics import confusion_matrix

# ... (your existing code)

# After you have predicted the target variable 'y_pred_nb'
conf_matrix = confusion_matrix(y_test, y_pred_nb, labels=[0, 1])

# Display the confusion matrix
class_names = ['No treatment', 'Treatment']
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.neural_network import MLPClassifier

# Load and preprocess your data (including encoding and scaling)

# Define and train your neural network model
mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=500, random_state=42)

# Fit the model
mlp.fit(X_train, y_train)

# Predict on the test set
y_pred_nn = mlp.predict(X_test)

# Convert the predictions to 0 and 1
y_pred_nn_binary = (y_pred_nn > 0.5).astype(int)

# Calculate accuracy
accuracy_nn = accuracy_score(y_true=y_test, y_pred=y_pred_nn_binary) * 100
print('Neural Network accuracy:', accuracy_nn)

# Calculate and display confusion matrix
conf_matrix_nn = confusion_matrix(y_test, y_pred_nn_binary)
print("Confusion Matrix:")
print(conf_matrix_nn)

# Display classification report
report_nn = classification_report(y_test, y_pred_nn_binary)
print(report_nn)

from sklearn.ensemble import RandomForestClassifier
import shap

# Define and train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on the test set
y_pred_rf = rf_model.predict(X_test)

accuracy_rf = accuracy_score(y_true=y_test, y_pred=y_pred_rf) * 100
print('Random Forest accuracy:', accuracy_rf)

shap.initjs()
ex_rf = shap.TreeExplainer(rf_model)
shap_values_rf = ex_rf.shap_values(X_test)

shap.summary_plot(shap_values_rf, X_test, max_display=30)

report_rf = classification_report(y_test, y_pred_rf)
print(report_rf)

# Predict using the Neural Network model
y_pred_nn = mlp.predict(X_test)

# Convert the predictions to 0 and 1
y_pred_nn_binary = (y_pred_nn > 0.5).astype(int)

# Predict using the Random Forest model
y_pred_rf = rf_model.predict(X_test)

import pandas as pd

# Assuming y_pred_nn_binary and y_pred_rf are the predictions
predictions_df = pd.DataFrame({'Neural_Network_Predictions': y_pred_nn_binary, 'Random_Forest_Predictions': y_pred_rf})

# Save the DataFrame to a CSV file
predictions_df.to_csv('predictions.csv', index=False)

predictions_df

!pip install lime shap

import lime
import lime.lime_tabular
import shap

explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=['No Treatment', 'Treatment'], discretize_continuous=True)

# Choose a specific instance from the test set for explanation
instance_index = 0  # Adjust the index as needed
instance = X_test.iloc[[instance_index]]

# Explain the prediction for the chosen instance using Lime
exp = explainer.explain_instance(instance.values[0], rf_model.predict_proba, num_features=len(X_train.columns))

# Visualize the explanation
exp.show_in_notebook(show_table=True, show_all=False)

explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=['No Treatment', 'Treatment'], discretize_continuous=True)

# Choose a specific instance from the test set for explanation
instance_index = 0  # Adjust the index as needed
instance = X_test.iloc[[instance_index]]

# Explain the prediction for the chosen instance using Lime
exp = explainer.explain_instance(instance.values[0], rf_model.predict_proba, num_features=len(X_train.columns))

# Visualize the explanation
exp.show_in_notebook(show_table=True, show_all=False)

# Initialize SHAP explainer for the Random Forest model
explainer_rf = shap.TreeExplainer(rf_model)

# Get SHAP values for the test set
shap_values_rf = explainer_rf.shap_values(X_test)

# Summary plot for feature importance
shap.summary_plot(shap_values_rf, X_test, max_display=30)

explainer_nn = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_train.columns, class_names=['No Treatment', 'Treatment'], discretize_continuous=True)

# Explain the prediction for the chosen instance using Lime
exp_nn = explainer_nn.explain_instance(instance.values[0], mlp.predict_proba, num_features=len(X_train.columns))

# Visualize the explanation
exp_nn.show_in_notebook(show_table=True, show_all=False)

import shap
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.pipeline import Pipeline

# ... (Previous code)

# Load and preprocess your data (including encoding and scaling)
# ...

# Define and train your neural network model
mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=500, random_state=42)

# Fit the model
mlp.fit(X_train, y_train)

# Predict on the test set
y_pred_nn = mlp.predict(X_test)

# Convert the predictions to 0 and 1
y_pred_nn_binary = (y_pred_nn > 0.5).astype(int)

# Calculate accuracy
accuracy_nn = accuracy_score(y_true=y_test, y_pred=y_pred_nn_binary) * 100
print('Neural Network accuracy:', accuracy_nn)

# Calculate and display confusion matrix
conf_matrix_nn = confusion_matrix(y_test, y_pred_nn_binary)
print("Confusion Matrix:")
print(conf_matrix_nn)

# Display classification report
report_nn = classification_report(y_test, y_pred_nn_binary)
print(report_nn)

# Use KernelExplainer from SHAP for the Neural Network model
explainer_nn = shap.KernelExplainer(model=lambda x: mlp.predict_proba(x)[:, 1], data=X_train.iloc[:100, :])
shap_values_nn = explainer_nn.shap_values(X_test.iloc[:10, :])

# Summary plot for feature importance
shap.summary_plot(shap_values_nn, X_test.iloc[:10, :], max_display=30)

# ... (previous code)

# Calculate accuracy for Naive Bayes
accuracy_nb = accuracy_score(y_true=y_test, y_pred=y_pred_nb) * 100
print('Naive Bayes accuracy:', accuracy_nb)

# ... (previous code)

# Calculate accuracy for Random Forest
accuracy_rf = accuracy_score(y_true=y_test, y_pred=y_pred_rf) * 100
print('Random Forest accuracy:', accuracy_rf)

# ... (previous code)

# Calculate accuracy for Neural Network
accuracy_nn = accuracy_score(y_true=y_test, y_pred=y_pred_nn_binary) * 100
print('Neural Network accuracy:', accuracy_nn)

# ... (previous code)

!pip install flask-ngrok

import joblib

import joblib

# Assuming your trained model is stored in a variable called 'model'
# Train your model...

# Save the model to a file
joblib.dump(rf_model, 'your_model.pkl')

"/your_project/"
"├── app.py"
print("""
├── templates/
│   └── index.html
├── static
│   └── (static files like CSS, JavaScript, etc.)
└── your_model.pkl
""")

import os
os.chdir('/content/drive/MyDrive/Colab Notebooks')

# Run your Flask application
!python mentalH.ipynb

from IPython.display import display, Javascript

# Increase data rate limit to 10 MB/sec (adjust as needed)
display(Javascript("Jupyter.notebook.kernel.execute('%%javascript\nJupyter.notebook.kernel.comm_manager.register_target_handler(\"iopub\", function(){}, {\"rate_limit\": 10000000})')"))

!python mentalH.ipynb

!pip install --upgrade ngrok

!pip install pyngrok

from flask import Flask, request, jsonify, render_template
from pyngrok import ngrok
import joblib

# Set your ngrok authentication token
ngrok.set_auth_token("2gP9ajhRTYxMx6QX0TrgCouCxfM_2JrpiXZeobAijqJiWm4nw")

# Running the Flask app
app = Flask(__name__)

# Load your trained model
model = joblib.load("/content/your_model.pkl")

# Connect to ngrok
ngrok_tunnel = ngrok.connect(5000)

# Define route to render input form
@app.route("/")
def index():
    return render_template("index.html")

# Define route to handle form submission and make prediction
@app.route("/predict", methods=["POST"])
def predict():
    # Get data from the form
    age = request.form.get("age")
    gender = request.form.get("gender")
    # Get other input fields similarly

    # Preprocess input (convert to numerical format, handle missing values, etc.)
    # Example: preprocess age and gender

    # Make prediction using the model
    input_data = [[age, gender]]  # Adjust according to your input format
    prediction = model.predict(input_data)

    # Format the prediction (e.g., convert to human-readable format)
    output = "Predicted class: {}".format(prediction)

    return render_template("prediction.html", output=output)

if __name__ == "__main__":
    try:
        print("Public URL:", ngrok_tunnel.public_url)
        app.run()
    except KeyboardInterrupt:
        print("Shutting down...")
    finally:
        ngrok_tunnel.close()

from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)

# Load your trained model
model = joblib.load("your_model.pkl")

@app.route("/predict", methods=["POST"])
def predict():
    # Get data from the request
    data = request.json

    # Preprocess data if necessary (e.g., convert to appropriate format for model input)
    # ...

    # Make predictions using the loaded model
    predictions = model.predict(data)

    # Return predictions as a JSON response
    return jsonify({"predictions": predictions.tolist()})

if __name__ == "__main__":
    app.run()